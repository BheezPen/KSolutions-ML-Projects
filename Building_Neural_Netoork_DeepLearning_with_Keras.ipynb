{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ee42fd7-6246-4dd7-b09e-4ee2d0c0ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import boston_housing #Importing the builting dataset to be used\n",
    "\n",
    "from sklearn import preprocessing #To help in processing the training and testing dateset  \n",
    "\n",
    "from keras.models import Sequential #This helps to crate a model\n",
    "\n",
    "from keras.layers import Dense #Dense is the name of the layers we would be using today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bdd557-1443-4faa-b7ca-dc2d9a47b64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Note, Keras do have 2 backend it can use, one is the Tensoflow, the other is Theano.\n",
    "### This backends, are import we know when working on a project with keras\n",
    "### The check the ackend here\n",
    "\n",
    "from keras import backend as bk\n",
    "bk.backend() #As below, i prints Tensorflow, telling thats the backend we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c4ec7f-f9b0-44b9-bd09-19caab99fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 5us/step\n"
     ]
    }
   ],
   "source": [
    "# Now lets import the boston_housing dataset\n",
    "\n",
    "(x_train, y_train) , (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8da544-f2b2-4fd5-bbae-0421fdf7e135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1622553c-cfb7-4802-a961-fa81c4e4559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c55d240-5838-4bef-9a04-49d6cf6eb5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80846e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        2.72500e+01, 2.90500e+01],\n",
       "       [1.23290e-01, 0.00000e+00, 1.00100e+01, ..., 1.78000e+01,\n",
       "        3.94950e+02, 1.62100e+01],\n",
       "       [5.49700e-02, 0.00000e+00, 5.19000e+00, ..., 2.02000e+01,\n",
       "        3.96900e+02, 9.74000e+00],\n",
       "       ...,\n",
       "       [1.83377e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        3.89610e+02, 1.92000e+00],\n",
       "       [3.58090e-01, 0.00000e+00, 6.20000e+00, ..., 1.74000e+01,\n",
       "        3.91700e+02, 9.71000e+00],\n",
       "       [2.92400e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.40160e+02, 9.81000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85730d0d-85f9-4ae1-b205-8c301b466971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.2, 18.8, 19. , 27. , 22.2, 24.5, 31.2, 22.9, 20.5, 23.2, 18.6,\n",
       "       14.5, 17.8, 50. , 20.8, 24.3, 24.2, 19.8, 19.1, 22.7, 12. , 10.2,\n",
       "       20. , 18.5, 20.9, 23. , 27.5, 30.1,  9.5, 22. , 21.2, 14.1, 33.1,\n",
       "       23.4, 20.1,  7.4, 15.4, 23.8, 20.1, 24.5, 33. , 28.4, 14.1, 46.7,\n",
       "       32.5, 29.6, 28.4, 19.8, 20.2, 25. , 35.4, 20.3,  9.7, 14.5, 34.9,\n",
       "       26.6,  7.2, 50. , 32.4, 21.6, 29.8, 13.1, 27.5, 21.2, 23.1, 21.9,\n",
       "       13. , 23.2,  8.1,  5.6, 21.7, 29.6, 19.6,  7. , 26.4, 18.9, 20.9,\n",
       "       28.1, 35.4, 10.2, 24.3, 43.1, 17.6, 15.4, 16.2, 27.1, 21.4, 21.5,\n",
       "       22.4, 25. , 16.6, 18.6, 22. , 42.8, 35.1, 21.5, 36. , 21.9, 24.1,\n",
       "       50. , 26.7, 25. ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f286ac-6143-4232-b7e8-488b4777ff2a",
   "metadata": {},
   "source": [
    "### SO, now beacuse data that's used in training of the neural networks are mostly very unstructured\n",
    "### Imagine a data from images to find pattern in the images so as to classify them\n",
    "### or data from videos\n",
    "### Surely they are so unstructured, and very random\n",
    "### to get the accuracy and predition somehow right, it will be necessary to use some sort of Processors & Transformers\n",
    "### i.e scikit-learn processor to be able to ensure that the training data and the test data are somewhat uniform. (i.e of same type)\n",
    "### This comformity and uniformity of both data is done before the learning and training process\n",
    "### Think of it like, before you starting test me with examination in school, we need to agree that before the teaching \n",
    "### So, with the agreement, i can followup with the teaching and training to prepare for examination (prediction in this case)\n",
    "### Between that is when the Uniforminity and Conformity comes in, the teaching must be uniform with the examination, right?\n",
    "### This process is very important in neural network beacause all data are quite random, so we have to ensure that thinkgs go as expected between the traning and testung data\n",
    "\n",
    "## All this process of reachign agreement between Training Data and Testing Data is called PROCESSING from sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07141944-43cb-4f64-b527-746ac0a8dcaf",
   "metadata": {},
   "source": [
    "# TO process the data here\n",
    "scale_x_train = preprocessing.scale(x_train) #THis scale out the x_train data\n",
    "\n",
    "#AFter scaling out, we convert it out to scaler\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "#The StandardScaler wil Standardize features by removing the mean and scaling to unit variance\n",
    "# return like the standdard deviation, mean deviation, \n",
    "# then the fit() Compute the mean and std to be used for later scaling\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b8e7eb1-2257-4de3-9cc8-e6340adc8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is to scale the x_test\n",
    "scale_x_test = scaler.transform(x_test)\n",
    "# SO, with all of this done, in a nutshell, the trainign data has been normalized with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28059e73-715e-4e70-9c30-2fca8157e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets create our model.\n",
    "\n",
    "my_model = Sequential() #Instantaiting the model from sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73beb6-4b0c-49ae-bb3d-7e8898d6ea99",
   "metadata": {},
   "source": [
    "## Now lets start adding layers to our model\n",
    "\n",
    "my_model.add(Dense(no_of_features, kernel_initializers, activation)) <br/>\n",
    "\n",
    "In this case, the add method adds a layer instance on top of the layer stack <br/>\n",
    "In this case, dense is the name of a layer, there are several of it. <br/>\n",
    "Here, using it as the input layer of the neural model <br/>\n",
    "The dense takes a parameter which serve as the number of features in the dataset.  <br/>\n",
    "Bcus in most cases, the number of features isn't constant <br/>\n",
    "and even because, in some cases, the dataset isnt even known, we dont see it, since its a neural network that learn by itself, <br/>\n",
    "Then how do we know the number of features??? <br/>\n",
    "Well, while thats the standard, there are other rule of thumb that tells what should be used, maybe 32, maybe 64 <br/>\n",
    "But in this case lets use 64 <br/>\n",
    "no_of_features = 64 <br/>\n",
    "\n",
    "## Adding a layer ##\n",
    "my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = )) <br/>\n",
    "kernel_initializer means that, when inputs are coming in in matrixs, its initilize the value of the them, it could be zero, or it could be one <br/>\n",
    "Some possible value for kernel_initializer variable is \"normal\" which is default,  another is 'glorot_uniform' suitable for most cases <br/>\n",
    "Another is 'he_normal' which is effective for deep networks with ReLU activation function (ReLU to be explained later)  <br/>\n",
    "\n",
    "## Another parameter is the activator, which help to determine the kind of inputs should be used or passed to the next neuron. ##\n",
    "After several data are coming in as inputs, in used in the inout layer, the activators check through them and pick which is to be picked depends on the kind of activator that is to be used. <br/>\n",
    "In neural networks, they are like a decision maker for each neuron in the network. \n",
    "it decides whether the neuron should pass information to the next layer of neurons <br/>\n",
    "From the several of this activator, we have the <br/>\n",
    "\n",
    "Sigmoid: Imagine it as a switch that can be either OFF (0) or ON (1). It's good for making yes/no decisions, like whether an email is spam or not. However, it's not great for very deep networks.\n",
    "\n",
    "ReLU (Rectified Linear Unit): Think of it as a light switch that's either OFF (0) or ON (positive number)It set all negative values to Zero and is computationally efficient. . It's the most popular because it's simple and works well in many cases, like recognizing shapes in image ReLU helps mitigate the vanishing gradient problem but can suffer from 'dying Relu' issues when neurons get stuck at Zero during training.\n",
    "\n",
    "Leaky ReLU: Similar to ReLU, but if the light switch is OFF, there's a tiny bit of light (a small positive number). it addresses the dying Relu problems by allowing a small gradient for negative values in deep networks, which helps avoid neurons becoming inactive.\n",
    "\n",
    "ELU (Exponential Linear Unit): Like Leaky ReLU, but the light switch being OFF isn't pitch black; it's a bit brighter helps training even better. it has a non-zero gradient for negative values which can help with convergence and robustness.\n",
    "\n",
    "Tanh (Hyperbolic Tangent): Imagine it like a thermometer that can show temperatures from -1 to 1. It's useful for data centered around zero, like when predicting if a stock will go up or down. though kinde suffers from vanishing gradients.\n",
    "\n",
    "Softmax: This one is like dividing a pie into slices, where each slice shows the probability of something happening (like each slice shows propability of you getting satisfied). It's often used to decide which of many things is most likely, its primarily used in the outlayer for multiclass classification.\n",
    "\n",
    "Swish: Think of Swish as a fancy light dimmer switch that works well in many situations but isn't as popular as ReLU. its kinda promising in lots of scenerio, and striking a balance between Relu and sigmoid-like functions.\n",
    "\n",
    "GELU (Gaussian Error Linear Unit): This one is like a special tool used in some situations, like understanding the meaning of words in a sentence. Its used in tranformer-based models and effective in NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b44bea0-f506-49bb-a81a-e234ec555d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_features = 64\n",
    "\n",
    "## Adding a layer ##\n",
    "#my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = 'ReLU' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9823aa4a-78f6-4b8c-80db-d449df46e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next after this is to define the input_shape, which is more like the shape of the data, this is quite debatable\n",
    "# we do that with the input_shape paramter to the Dense class we are filling\n",
    "# to have an idea of our data shape we can easily use the .shape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e48e9302-e556-4ecf-a5c7-df8bfb6030ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7db838ac-d175-473c-aa2c-1dacfc74e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHile the shape result doesnt always dictate the value for the input_shape parameter\n",
    "# it could be a factor, so for this use case, i will use 13\n",
    "# Note that there use to be a CV function that could be imported which is specifically used for getting the efficient shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edf72cc7-1aa9-4ad6-9d96-d0d918be155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding the input layer ##\n",
    "\n",
    "my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = 'ReLU', input_shape = ('13', )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3577a83-96d7-4641-990d-45c7431b2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding a hidden layer 1\n",
    "my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = 'ReLU', input_shape = ('13', )))\n",
    "\n",
    "# It might feel to ask that how many hidden layer are enough after the input layer and before the output layer\n",
    "# Well, while the numbers of hidden layer may depend on the problem being solved, we should also consider that\n",
    "# in some cases, the neural network are not built for specific problem.\n",
    "# Though, what happens behind the scene is that:\n",
    "# When there are enough hidden layer, if a problem is being solved\n",
    "# There is a function that monitors if the problem is satisfactoryly solve\n",
    "# Then it automatically moved the solution to the output layer \n",
    "# So, knowning this, there may not be specific value for hidden layer,\n",
    "# but being good enough is good, though might require higher resources.\n",
    "# at time when accuracy is low, it could be decided to add more hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508ff24-9758-4e3e-b563-4460b984ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we are building this model from stratch is why we require all of this\n",
    "# there are already built model made avaiable.\n",
    "# EG, the popular ChatGPT uses the transformer model.\n",
    "# And there are always different model built for different purpose\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
