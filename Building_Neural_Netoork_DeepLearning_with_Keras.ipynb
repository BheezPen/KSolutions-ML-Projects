{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ee42fd7-6246-4dd7-b09e-4ee2d0c0ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import boston_housing #Importing the builting dataset to be used\n",
    "\n",
    "from sklearn import preprocessing #To help in processing the training and testing dateset  \n",
    "\n",
    "from keras.models import Sequential #This helps to crate a model\n",
    "\n",
    "from keras.layers import Dense #Dense is the name of the layers we would be using today\n",
    "\n",
    "from keras.optimizers import RMSprop #imported to helo \n",
    "\n",
    "from keras.callbacks import EarlyStopping #THis is used for the callback, i.e to stop inputs going through other hidden layers after mision is achived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bdd557-1443-4faa-b7ca-dc2d9a47b64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Note, Keras do have 2 backend it can use, one is the Tensoflow, the other is Theano.\n",
    "### This backends, are import we know when working on a project with keras\n",
    "### The check the ackend here\n",
    "\n",
    "from keras import backend as bk\n",
    "bk.backend() #As below, i prints Tensorflow, telling thats the backend we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c4ec7f-f9b0-44b9-bd09-19caab99fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 5us/step\n"
     ]
    }
   ],
   "source": [
    "# Now lets import the boston_housing dataset\n",
    "\n",
    "(x_train, y_train) , (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8da544-f2b2-4fd5-bbae-0421fdf7e135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1622553c-cfb7-4802-a961-fa81c4e4559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c55d240-5838-4bef-9a04-49d6cf6eb5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80846e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        2.72500e+01, 2.90500e+01],\n",
       "       [1.23290e-01, 0.00000e+00, 1.00100e+01, ..., 1.78000e+01,\n",
       "        3.94950e+02, 1.62100e+01],\n",
       "       [5.49700e-02, 0.00000e+00, 5.19000e+00, ..., 2.02000e+01,\n",
       "        3.96900e+02, 9.74000e+00],\n",
       "       ...,\n",
       "       [1.83377e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        3.89610e+02, 1.92000e+00],\n",
       "       [3.58090e-01, 0.00000e+00, 6.20000e+00, ..., 1.74000e+01,\n",
       "        3.91700e+02, 9.71000e+00],\n",
       "       [2.92400e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.40160e+02, 9.81000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85730d0d-85f9-4ae1-b205-8c301b466971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.2, 18.8, 19. , 27. , 22.2, 24.5, 31.2, 22.9, 20.5, 23.2, 18.6,\n",
       "       14.5, 17.8, 50. , 20.8, 24.3, 24.2, 19.8, 19.1, 22.7, 12. , 10.2,\n",
       "       20. , 18.5, 20.9, 23. , 27.5, 30.1,  9.5, 22. , 21.2, 14.1, 33.1,\n",
       "       23.4, 20.1,  7.4, 15.4, 23.8, 20.1, 24.5, 33. , 28.4, 14.1, 46.7,\n",
       "       32.5, 29.6, 28.4, 19.8, 20.2, 25. , 35.4, 20.3,  9.7, 14.5, 34.9,\n",
       "       26.6,  7.2, 50. , 32.4, 21.6, 29.8, 13.1, 27.5, 21.2, 23.1, 21.9,\n",
       "       13. , 23.2,  8.1,  5.6, 21.7, 29.6, 19.6,  7. , 26.4, 18.9, 20.9,\n",
       "       28.1, 35.4, 10.2, 24.3, 43.1, 17.6, 15.4, 16.2, 27.1, 21.4, 21.5,\n",
       "       22.4, 25. , 16.6, 18.6, 22. , 42.8, 35.1, 21.5, 36. , 21.9, 24.1,\n",
       "       50. , 26.7, 25. ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f286ac-6143-4232-b7e8-488b4777ff2a",
   "metadata": {},
   "source": [
    "### SO, now beacuse data that's used in training of the neural networks are mostly very unstructured\n",
    "### Imagine a data from images to find pattern in the images so as to classify them\n",
    "### or data from videos\n",
    "### Surely they are so unstructured, and very random\n",
    "### to get the accuracy and predition somehow right, it will be necessary to use some sort of Processors & Transformers\n",
    "### i.e scikit-learn processor to be able to ensure that the training data and the test data are somewhat uniform. (i.e of same type)\n",
    "### This comformity and uniformity of both data is done before the learning and training process\n",
    "### Think of it like, before you starting test me with examination in school, we need to agree that before the teaching \n",
    "### So, with the agreement, i can followup with the teaching and training to prepare for examination (prediction in this case)\n",
    "### Between that is when the Uniforminity and Conformity comes in, the teaching must be uniform with the examination, right?\n",
    "### This process is very important in neural network beacause all data are quite random, so we have to ensure that thinkgs go as expected between the traning and testung data\n",
    "\n",
    "## All this process of reachign agreement between Training Data and Testing Data is called PROCESSING from sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07141944-43cb-4f64-b527-746ac0a8dcaf",
   "metadata": {},
   "source": [
    "# TO process the data here\n",
    "scale_x_train = preprocessing.scale(x_train) #THis scale out the x_train data\n",
    "\n",
    "#AFter scaling out, we convert it out to scaler\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "#The StandardScaler wil Standardize features by removing the mean and scaling to unit variance\n",
    "# return like the standdard deviation, mean deviation, \n",
    "# then the fit() Compute the mean and std to be used for later scaling\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b8e7eb1-2257-4de3-9cc8-e6340adc8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is to scale the x_test\n",
    "scale_x_test = scaler.transform(x_test)\n",
    "# SO, with all of this done, in a nutshell, the trainign data has been normalized with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28059e73-715e-4e70-9c30-2fca8157e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets create our model.\n",
    "\n",
    "my_model = Sequential() #Instantaiting the model from sequential\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05d7bd92-66d6-445b-81a4-eaeb6e420a67",
   "metadata": {},
   "source": [
    "## Now lets start adding layers to our model\n",
    "\n",
    "my_model.add(Dense(no_of_features, kernel_initializers, activation)) <br/>\n",
    "\n",
    "In this case, the add method adds a layer instance on top of the layer stack <br/>\n",
    "In this case, dense is the name of a layer, there are several of it. <br/>\n",
    "Here, using it as the input layer of the neural model <br/>\n",
    "The dense takes a parameter which serve as the number of features in the dataset.  <br/>\n",
    "Bcus in most cases, the number of features isn't constant <br/>\n",
    "and even because, in some cases, the dataset isnt even known, we dont see it, since its a neural network that learn by itself, <br/>\n",
    "Then how do we know the number of features??? <br/>\n",
    "Well, while thats the standard, there are other rule of thumb that tells what should be used, maybe 32, maybe 64 <br/>\n",
    "But in this case lets use 64 <br/>\n",
    "no_of_features = 64 <br/>\n",
    "\n",
    "## Adding a layer ##\n",
    "my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = )) <br/>\n",
    "kernel_initializer means that, when inputs are coming in in matrixs, its initilize the value of the them, it could be zero, or it could be one <br/>\n",
    "Some possible value for kernel_initializer variable is \"normal\" which is default,  another is 'glorot_uniform' suitable for most cases <br/>\n",
    "Another is 'he_normal' which is effective for deep networks with ReLU activation function (ReLU to be explained later)  <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81652c-8920-454c-bcd4-303861a91c33",
   "metadata": {},
   "source": [
    "## Another parameter is the activator, which help to determine the kind of inputs should be used or passed to the next neuron. ##\n",
    "After several data are coming in as inputs, in used in the inout layer, the activators check through them and pick which is to be picked depends on the kind of activator that is to be used. <br/>\n",
    "In neural networks, they are like a decision maker for each neuron in the network. \n",
    "it decides whether the neuron should pass information to the next layer of neurons <br/>\n",
    "From the several of this activator, we have the <br/>\n",
    "\n",
    "Sigmoid: Imagine it as a switch that can be either OFF (0) or ON (1). It's good for making yes/no decisions, like whether an email is spam or not. However, it's not great for very deep networks.\n",
    "\n",
    "ReLU (Rectified Linear Unit): Think of it as a light switch that's either OFF (0) or ON (positive number)It set all negative values to Zero and is computationally efficient. . It's the most popular because it's simple and works well in many cases, like recognizing shapes in image ReLU helps mitigate the vanishing gradient problem but can suffer from 'dying Relu' issues when neurons get stuck at Zero during training.\n",
    "\n",
    "Leaky ReLU: Similar to ReLU, but if the light switch is OFF, there's a tiny bit of light (a small positive number). it addresses the dying Relu problems by allowing a small gradient for negative values in deep networks, which helps avoid neurons becoming inactive.\n",
    "\n",
    "ELU (Exponential Linear Unit): Like Leaky ReLU, but the light switch being OFF isn't pitch black; it's a bit brighter helps training even better. it has a non-zero gradient for negative values which can help with convergence and robustness.\n",
    "\n",
    "Tanh (Hyperbolic Tangent): Imagine it like a thermometer that can show temperatures from -1 to 1. It's useful for data centered around zero, like when predicting if a stock will go up or down. though kinde suffers from vanishing gradients.\n",
    "\n",
    "Softmax: This one is like dividing a pie into slices, where each slice shows the probability of something happening (like each slice shows propability of you getting satisfied). It's often used to decide which of many things is most likely, its primarily used in the outlayer for multiclass classification.\n",
    "\n",
    "Swish: Think of Swish as a fancy light dimmer switch that works well in many situations but isn't as popular as ReLU. its kinda promising in lots of scenerio, and striking a balance between Relu and sigmoid-like functions.\n",
    "\n",
    "GELU (Gaussian Error Linear Unit): This one is like a special tool used in some situations, like understanding the meaning of words in a sentence. Its used in tranformer-based models and effective in NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3dcfab90-ad38-4593-aa53-4b6f0c50aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_of_features = 64 \n",
    "# This is same as number of neurons\n",
    "\n",
    "## Adding a layer ##\n",
    "#my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = 'ReLU' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9823aa4a-78f6-4b8c-80db-d449df46e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next after this is to define the input_shape, which is more like the shape of the data, this is quite debatable\n",
    "# we do that with the input_shape paramter to the Dense class we are filling\n",
    "# to have an idea of our data shape we can easily use the .shape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e48e9302-e556-4ecf-a5c7-df8bfb6030ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fbd24-0677-4239-b219-ce4c4ba48ad9",
   "metadata": {},
   "source": [
    "### WHile the shape result doesnt always dictate the value for the input_shape parameter\n",
    "### it could be a factor, so for this use case, i will use 13\n",
    "### Note that there use to be a CV function that could be imported which is specifically used for getting the efficient shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "edf72cc7-1aa9-4ad6-9d96-d0d918be155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding the input layer ##\n",
    "no_of_features = 64 \n",
    "\n",
    "my_model.add(Dense(no_of_features, kernel_initializer = 'normal', activation = 'ReLU', input_shape = (13, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3577a83-96d7-4641-990d-45c7431b2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding a hidden layer 1\n",
    "my_model.add(Dense(no_of_features, activation = 'ReLU'))\n",
    "# SO, when creating the new layer, kernel initializer are not needed to be passed\n",
    "# and the input_shape are not needed, thats because the output of the input layer becomes the input of the next hidden layer\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c88caaf-4603-4d63-bedb-2a7216d189bf",
   "metadata": {},
   "source": [
    "It might feel to ask that how many hidden layer are enough after the input layer and before the output layer\n",
    "Well, while the numbers of hidden layer may depend on the problem being solved, we should also consider that in some cases, the neural network are not built for specific problem.\n",
    "Though, what happens behind the scene is that:\n",
    "When there are enough hidden layer, if a problem is being solved\n",
    "There is a function that monitors if the problem is satisfactoryly solve\n",
    "Then it automatically moved the solution to the output layer \n",
    "So, knowning this, there may not be specific value for hidden layer,\n",
    "but being good enough is good, though might require higher resources.\n",
    "at time when accuracy is low, it could be decided to add more hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717896eb-f897-4a5f-971d-b284115407ee",
   "metadata": {},
   "source": [
    "### Because we are building this model from stratch is why we require all of this\n",
    "### there are already built model made avaiable.\n",
    "### EG, the popular ChatGPT uses the transformer model.\n",
    "### And there are always different model built for different purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "769f013d-3760-4084-b1fc-e94aca524270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets create for more hidden layers\n",
    "\n",
    "## Adding a hidden layer 2\n",
    "my_model.add(Dense(no_of_features, activation = 'ReLU'))\n",
    "\n",
    "\n",
    "## Adding a hidden layer 3\n",
    "my_model.add(Dense(no_of_features, activation = 'ReLU'))\n",
    "\n",
    "\n",
    "## Adding a hidden layer 4\n",
    "my_model.add(Dense(no_of_features, activation = 'ReLU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f86e98a3-f5ab-426e-affd-c2fa2c7f0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add the outer layer\n",
    "\n",
    "\n",
    "## Adding a Outer Layer\n",
    "my_model.add(Dense(1))\n",
    "# For the output model, the no_of_features (i.e no_of_neurons) is mostly 1, especialy for regression models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b6cd26d-03a2-4618-bae7-5e5e43984216",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next is to compile the model\n",
    "### Compiling the model is needed after building the model\n",
    "### We compile by saying\n",
    "\n",
    "my_model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer = RMSprop(),\n",
    "    metrics = ['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4440fadc-c11f-4984-8308-28608511cd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.03125"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO get the epoch value that will be needed below\n",
    "size = x_train.size\n",
    "\n",
    "size\n",
    "\n",
    "epoch_value = size / 128 #(i.e 5252/128)\n",
    "epoch_value"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7eb1053-dc82-4a56-bdaf-e4cda1e393ed",
   "metadata": {},
   "source": [
    "\n",
    "# Training the model, just like its done when we were using only sklearn\n",
    "training = my_model.fit(\n",
    "    scale_x_train, #This should be the x train data that has been preprocessed and matched the test data\n",
    "    y_train, # The y train data\n",
    "    batch_size = 128, #This store the number of samples from dataset that wil the trained at same time, normal popular is mostly 128\n",
    "    # SO, lets use 128 also. if batch_size isn't set, its default is 32\n",
    "    epochs = 500, # The number of iterate (i.e runs) the processing should be done is called Epoch\n",
    "    # Epochs could be calculated by size of dataset divided by batch_size (i.e epoch = size / batch_size)\n",
    "    verbose = 1, # OPTIONAL PARAMETER: verbose is the logging level of the training\n",
    "    # It  takes 0, 1 or 2, default is 1, which monitor the training progree\n",
    "    # 0  means does not monitor the training  progress, it \n",
    "    validation_split = 0.2, # This works like dividing the dataset into training and testing,\n",
    "    # If it's 0.3, then means like, 30% is for testing ahile automatically 70% for training\n",
    "    #If it's 0.2, then means like, 20% is for testing ahile automatically 80% for training\n",
    "    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 20)]\n",
    ") \n",
    "Callback paramater for the fit function means to specify when the model should stop sending inputs to the next hidden layer but rather sends it to the output layer if it has already solve the problem and well\n",
    "TO use it, we need to import the EarlyStopping from Keras.callbacks  then use in the syntax above\n",
    "#patience parameter for EarlStopping means how long it should wait to conclude that the input has been solved and can be sent to the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d3f7688c-b1fe-4565-89a7-1d839f5fa9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 10s 512ms/step - loss: 664.4229 - mean_absolute_error: 24.0430 - val_loss: 657.9619 - val_mean_absolute_error: 23.9466\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 579.5751 - mean_absolute_error: 22.2540 - val_loss: 639.1177 - val_mean_absolute_error: 23.5499\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 563.7722 - mean_absolute_error: 21.9028 - val_loss: 622.5410 - val_mean_absolute_error: 23.1952\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 544.6367 - mean_absolute_error: 21.4605 - val_loss: 578.5188 - val_mean_absolute_error: 22.2260\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 480.8785 - mean_absolute_error: 19.8779 - val_loss: 391.4559 - val_mean_absolute_error: 17.5195\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 240.4680 - mean_absolute_error: 12.3646 - val_loss: 84.7131 - val_mean_absolute_error: 6.9011\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 85.4241 - mean_absolute_error: 6.7986 - val_loss: 88.0647 - val_mean_absolute_error: 7.4548\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 102.0314 - mean_absolute_error: 7.6907 - val_loss: 91.2789 - val_mean_absolute_error: 6.5773\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 84.3082 - mean_absolute_error: 6.5416 - val_loss: 84.8979 - val_mean_absolute_error: 6.6541\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 88.3595 - mean_absolute_error: 6.9995 - val_loss: 133.1931 - val_mean_absolute_error: 8.1972\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 92.1550 - mean_absolute_error: 6.9998 - val_loss: 86.1193 - val_mean_absolute_error: 6.5705\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 84.1182 - mean_absolute_error: 6.6365 - val_loss: 89.6344 - val_mean_absolute_error: 6.5648\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 85.3231 - mean_absolute_error: 6.6618 - val_loss: 85.7236 - val_mean_absolute_error: 7.1223\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 95.6529 - mean_absolute_error: 7.3839 - val_loss: 117.5928 - val_mean_absolute_error: 7.4964\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 101.2374 - mean_absolute_error: 7.7184 - val_loss: 90.8399 - val_mean_absolute_error: 6.5741\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 85.0343 - mean_absolute_error: 6.6763 - val_loss: 84.6353 - val_mean_absolute_error: 6.7055\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 93.7513 - mean_absolute_error: 7.0993 - val_loss: 109.1244 - val_mean_absolute_error: 7.1202\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 86.1058 - mean_absolute_error: 6.4986 - val_loss: 84.5477 - val_mean_absolute_error: 6.7423\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 87.8395 - mean_absolute_error: 6.8999 - val_loss: 134.8546 - val_mean_absolute_error: 8.2715\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 106.0624 - mean_absolute_error: 7.6960 - val_loss: 86.5511 - val_mean_absolute_error: 6.5606\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 91.3473 - mean_absolute_error: 7.0002 - val_loss: 127.1050 - val_mean_absolute_error: 7.9217\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 93.1525 - mean_absolute_error: 6.9005 - val_loss: 85.6256 - val_mean_absolute_error: 6.5860\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 87.0354 - mean_absolute_error: 7.0064 - val_loss: 90.5806 - val_mean_absolute_error: 6.5722\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 84.8918 - mean_absolute_error: 6.6047 - val_loss: 85.0756 - val_mean_absolute_error: 6.9957\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 89.5414 - mean_absolute_error: 7.0049 - val_loss: 119.6658 - val_mean_absolute_error: 7.5883\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 95.7138 - mean_absolute_error: 7.0759 - val_loss: 85.0911 - val_mean_absolute_error: 6.9989\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 99.5843 - mean_absolute_error: 7.5338 - val_loss: 126.4255 - val_mean_absolute_error: 7.8914\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 102.3606 - mean_absolute_error: 7.3946 - val_loss: 88.5355 - val_mean_absolute_error: 6.5552\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 84.8052 - mean_absolute_error: 6.6240 - val_loss: 84.5724 - val_mean_absolute_error: 6.7289\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 91.8133 - mean_absolute_error: 7.0918 - val_loss: 85.0389 - val_mean_absolute_error: 6.6363\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 85.7709 - mean_absolute_error: 6.6938 - val_loss: 109.5047 - val_mean_absolute_error: 7.1376\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 84.9308 - mean_absolute_error: 6.6253 - val_loss: 87.7702 - val_mean_absolute_error: 7.4183\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 105.7729 - mean_absolute_error: 7.7889 - val_loss: 99.6036 - val_mean_absolute_error: 6.7533\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 85.5840 - mean_absolute_error: 6.6481 - val_loss: 84.7863 - val_mean_absolute_error: 6.6724\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 89.5945 - mean_absolute_error: 6.8996 - val_loss: 113.9760 - val_mean_absolute_error: 7.3355\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 93.0898 - mean_absolute_error: 6.9606 - val_loss: 85.6048 - val_mean_absolute_error: 7.1013\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 87.6099 - mean_absolute_error: 6.9248 - val_loss: 90.4120 - val_mean_absolute_error: 6.5709\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 84.0950 - mean_absolute_error: 6.5839 - val_loss: 87.5745 - val_mean_absolute_error: 6.5506\n"
     ]
    }
   ],
   "source": [
    "# Training the model, just like its done when we were using only sklearn\n",
    "training = my_model.fit(\n",
    "    scale_x_train,\n",
    "    y_train, \n",
    "    batch_size = 128,\n",
    "    epochs = 500,\n",
    "    verbose = 1, \n",
    "    validation_split = 0.2,\n",
    "    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 20)]\n",
    ")\n",
    "\n",
    "# After running this, we need to take note that the value for the loss is reducing and also, the mean_absolute_error value is reducing\n",
    "# with that it tells that the model is learning properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dcb007-a06c-462a-9065-56f1303245d3",
   "metadata": {},
   "source": [
    "### hwew now, i notice that the loss function reduced enough, the mean_absolute_error reduced enough and then all othe reduced\n",
    "### that shows that the model learned properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "10ee6efe-b44e-459b-9dba-83396a5fa486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is to evaluate the model\n",
    "score = my_model.evaluate(scale_x_test, y_test, verbose = 0)\n",
    "# notice here that the x parameter takes the x_test thats has been scaled to fit the condiions as explained earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ce7698fc-bcf2-4997-b1a7-de9459561d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84.63633728027344, 6.513787746429443]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score\n",
    "#This echos a list with 2 parameters, first is the loss function value, while the other is the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "301eb27c-46c5-4dee-81f0-6d2f793b47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss  84.63633728027344\n"
     ]
    }
   ],
   "source": [
    "print('Loss is ', score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c640a4f2-941b-4587-8f77-593d3eb8de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  6.513787746429443\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy is ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "799c6dd4-0f03-4bf9-af2f-aa2071486c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# Now, lets use the model to predict\n",
    "prediction = my_model.predict(scale_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0b12dc9d-035d-46e3-b438-a5525a178e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.8984  ]\n",
      " [21.8984  ]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.8984  ]\n",
      " [21.8984  ]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.8984  ]\n",
      " [21.8984  ]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]\n",
      " [21.898397]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8dbdfe29-325a-4873-8b4c-0c4dbbe8c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.8984   21.8984   21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.8984\n",
      " 21.8984   21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.8984   21.8984   21.898397 21.898397\n",
      " 21.898397 21.898397 21.898397 21.898397]\n"
     ]
    }
   ],
   "source": [
    "# Since the prediction look long, lets flatten it\n",
    "\n",
    "print(prediction.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8922537-ceaf-4024-85fe-a272655b0f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
